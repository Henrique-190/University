{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as utils\n",
    "import holidays\n",
    "import re\n",
    "from collections import Counter\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,plot_confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import boxcox, boxcox_normmax, probplot\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def import_data(file):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_010_decay_power_099(current_iter): #2\n",
    "    base_learning_rate = 0.1\n",
    "    lr = base_learning_rate  * np.power(.99, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "def learning_rate_010_decay_power_0995(current_iter): #1\n",
    "    base_learning_rate = 0.1\n",
    "    lr = base_learning_rate  * np.power(.995, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "def learning_rate_005_decay_power_099(current_iter): #3\n",
    "    base_learning_rate = 0.05\n",
    "    lr = base_learning_rate  * np.power(.99, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEITURA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.61 MB\n",
      "Memory usage after optimization is: 0.15 MB\n",
      "Decreased by 75.8%\n",
      "Memory usage of dataframe is 0.14 MB\n",
      "Memory usage after optimization is: 0.03 MB\n",
      "Decreased by 74.9%\n"
     ]
    }
   ],
   "source": [
    "incidentes = import_data('../output/incidentes_tratado.csv')\n",
    "teste = import_data('../output/teste_tratado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_scalers = {}\n",
    "columns_to_normalize = ['magnitude_of_delay','delay_in_seconds','luminosity','avg_temperature','avg_atm_pressure','avg_humidity',\n",
    "'avg_wind_speed','avg_rain','month','day_of_week','day_of_year','hour','National','Regional', 'season']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "# treino\n",
    "for col in columns_to_normalize:\n",
    "    # fit\n",
    "    scaler_col = min_max_scaler.fit(incidentes[[col]])\n",
    "    # transform\n",
    "    incidentes[[col]] = min_max_scaler.transform(incidentes[[col]])\n",
    "    teste[[col]] = min_max_scaler.transform(teste[[col]])\n",
    "     # guardar scaler no dict\n",
    "    dict_scalers[col] = scaler_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divis√£o do target\n",
    "x = incidentes.drop(['incidents'], axis=1)\n",
    "y = incidentes['incidents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 01:18:56.990905: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-20 01:18:57.360425: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-20 01:18:57.360446: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-20 01:18:59.027765: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-20 01:18:59.027935: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-20 01:18:59.027948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-20 01:19:00.030427: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-20 01:19:00.030662: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-20 01:19:00.030711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (purp): /proc/driver/nvidia/version does not exist\n",
      "2022-12-20 01:19:00.031635: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 2)                 6         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Import python libraries required in this example:\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "# Use numpy arrays to store inputs (x) and outputs (y):\n",
    "x = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([[0], [1], [1], [0]]) \n",
    "\n",
    "# Define the network model and its arguments. \n",
    "# Set the number of neurons/nodes for each layer:\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_shape=(2,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid')) \n",
    "\n",
    "# Compile the model and calculate its accuracy:\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy']) \n",
    "\n",
    "# Print a summary of the Keras model:\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset_COMPETICAO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af40022f6b24da7d0ec2d2ff3772b63b87a9d2e288a5206d322010ec73befb1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
